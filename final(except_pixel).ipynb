{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c08cc793-e35f-4fa5-bd6e-d6d92c7458df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "from pydicom import dcmread\n",
    "from pydicom.dataset import Dataset\n",
    "from pydicom.sequence import Sequence\n",
    "from pydicom.data import get_testdata_file\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from shutil import copyfile\n",
    "from pydicom.valuerep import IS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ef6658-8319-49ce-a19c-aaa90339febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#需要remove的tags(不包含需要删除部分内容的tag)\n",
    "tags_to_clear = [\n",
    "    (0x0008, 0x0050),           #Accession Number\n",
    "    (0x0008, 0x0054),           #Retrieve AE Title    \n",
    "    (0x0008, 0x0080),           #Institution Name\n",
    "    (0x0008, 0x0081),           #Institution Address\n",
    "    (0x0008, 0x0090),           #Referring Physician Name\n",
    "    (0x0008, 0x0201),           #Timezone Offset From UTC \n",
    "    (0x0008, 0x1010),           #Station Name\n",
    "    (0x0008, 0x1048),           #Physician(s) of Record \n",
    "    (0x0008, 0x1050),           #Performing Physician's Name \n",
    "    (0x0008, 0x1070),           #Operators' Name \n",
    "    (0x0008, 0x1120),           #Referenced Patient Sequence\n",
    "    (0x0010, 0x0030),            #Patient Birth Date\n",
    "    (0x0010, 0x1000),\n",
    "    (0x0010, 0x1040),   \n",
    "    (0x0012, 0x0020),\n",
    "    (0x0012, 0x0021), \n",
    "    (0x0012, 0x0030),\n",
    "    (0x0012, 0x0031),\n",
    "    (0x0012, 0x0040),\n",
    "    (0x0012, 0x0042),\n",
    "    (0x0012, 0x0050),  \n",
    "    (0x0018, 0x1200),\n",
    "    (0x0020, 0x0010),\n",
    "    (0x0032, 0x1021),\n",
    "    (0x0040, 0x0009),\n",
    "    (0x0040, 0x0241),\n",
    "    (0x0040, 0x0275),\n",
    "    (0x0070, 0x0084),\n",
    "    (0x0088, 0x0220),\n",
    "    (0x0040, 0xA075),\n",
    "    (0x0040, 0xA123),\n",
    "    (0x0012, 0x0051),\n",
    "    (0x0043, 0x1005),\n",
    "    (0x0043, 0x1029),\n",
    "    (0x0043, 0x1060),\n",
    "    (0x0043, 0x1080),\n",
    "    (0x0009, 0x1002),\n",
    "    (0x0009, 0x1030),\n",
    "    (0x0009, 0x1037),\n",
    "    (0x0018, 0xa001),\n",
    "    (0x0008, 0x1040),\n",
    "    (0x0029, 0x1131),\n",
    "    (0x0029, 0x1134),\n",
    "    (0x0040, 0xA027),\n",
    "    (0x0040, 0xA073),\n",
    "    (0x0040, 0xA730),\n",
    "    (0x0021, 0x1035),\n",
    "    (0x0021, 0x1003),\n",
    "    (0x0040, 0x1001),\n",
    "    (0x0032, 0x1020),\n",
    "    (0x0040, 0x0242),\n",
    "    (0x0040, 0x0280),\n",
    "    (0x0040, 0x2001),\n",
    "]\n",
    "tags_to_shift = [0x00080012, 0x00080020, 0x00080021,0x00080022,0x00080023,0x00080024,0x00080025,\n",
    "                 0x0008002A,0x001021D0,0x00181012,0x0018700C,0x00380020,0x00400002,0x00400004,\n",
    "                 0x00400244,0x00400250,0x0040A032,\n",
    "                 0x00091005,0x0009100D,0x0009100E,\n",
    "                 0x00540016,0x00540410,0x00540414,0x0040a730,0x0040a073,0x0018a001,0x0018a002,0x0019109d,0x00091039,0x0009103b,0x0009103d,0x00091068,0x0009106c,0x0009107b,0x000910e9,0x00171004,#discrepancy,\n",
    "                 0x00181078,0x00181079,0x0040a030,0x0040a120,0x0040a121,0x00540300,0x00540412,0x00080106,0x00191010,0x00321040,0x00321050\n",
    "                ]\n",
    "tags_to_hash = [0x0020000D,0x0020000E,0x00200052,0x00200200,\n",
    "                0x0040A124,0x00880140,0x00400555,0x0040A073,0x0040A730,\n",
    "                0x00080014,0x00080018,0x00080118,0x00081155,0x00081120,0x00083010,\n",
    "                0x00081110,0x00081111,0x00081140,0x00082112,0x00081250,0x00089121,\n",
    "                0x00400513,0x00400562,0x00400610,0x00340001,0x00081084,0x0009100A,0x00091013,0x00091056,0x00091057,0x00091059,0x0009105C,0x0009105D,0x0009105E,0x00091098,\n",
    "                0x00091097,0x000910AD,0x00091007,0x000910e3,0x00431088,0x00431098,0x00451050,0x00451051,0x0008010c,   #discrepancy\n",
    "                0x00080110,0x0040a375,0x00081199,0x0040a504,0x00081115]     #discrepancy sequence\n",
    "tag_to_add = [0x00080068,0x00180060,0x00185101,0x00187004,0x00200012,0x00201040,0x20500020]\n",
    "tag_not_null = [0x00080068,0x00181000,0x00200062,0x20500020,0x00402001]\n",
    "sensitive_tags = [0x00081030,0x0008103e,0x001021b0,0x00102000,0x00104000,0x00181030,0x00184000,0x00204000,\n",
    "                  0x00321060,0x00324000,0x011710c4,0x01171024,0x00400275,0x00400007,0x00401400,0x00400310,0x00200011,0x0040a730,0x0040a160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "651acec6-897f-4f93-bd16-866d85203ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#为了满足text_notnull，在tag_not_null中的tag值为空时，置为'NA'\n",
    "def set_empty_tags_to_add(ds, tags):\n",
    "    for tag in tags:\n",
    "        tag = pydicom.tag.Tag(tag)  # Convert tag to pydicom.Tag object if necessary\n",
    "        if tag in ds:\n",
    "            value = ds[tag].value\n",
    "\n",
    "            # Print the original value for debugging\n",
    "            print(f\"Original value for tag {tag}: {value}\")\n",
    "\n",
    "            # Check if the value is truly empty\n",
    "            if value is None or (isinstance(value, str) and value.strip() == '') or (isinstance(value, list) and len(value) == 0):\n",
    "                ds[tag].value = \"NA\"\n",
    "                print(f\"Modified value for tag {tag} to 'NA'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dcbf8a4-0751-4772-af10-f17c69dc22d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_id_mapping(ds, id_lookup):\n",
    "    \"\"\"处理 PatientID 和 PatientName 的映射\"\"\"\n",
    "    if 'PatientID' in ds:\n",
    "        original_id = ds.PatientID\n",
    "        new_id = id_lookup.get(original_id, original_id)  # 使用映射的值，若无映射则使用原值\n",
    "        ds.PatientID = new_id\n",
    "\n",
    "def modify_patient_name_to_id(ds):\n",
    "    if 'PatientID' in ds and 'PatientName' in ds:\n",
    "        patient_id = ds.PatientID\n",
    "        ds.PatientName = patient_id  # 将 PatientName 设置为 PatientID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2c8a939-3c75-46ba-99b5-db8b81fa9746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_tags(ds, tags_to_clear):\n",
    "    if isinstance(ds, Dataset):\n",
    "        for elem in ds:\n",
    "            if isinstance(elem.value, Sequence):\n",
    "                # 处理序列中的每一个项\n",
    "                for item in elem.value:\n",
    "                    if isinstance(item, Dataset):\n",
    "                        # 递归处理数据集\n",
    "                        clear_tags(item, tags_to_clear)\n",
    "            elif isinstance(elem.value, Dataset):\n",
    "                # 递归处理嵌套数据集\n",
    "                clear_tags(elem.value, tags_to_clear)\n",
    "            elif elem.tag in tags_to_clear:\n",
    "                # 清除最底层标签的值\n",
    "                try:\n",
    "                    # 处理不同的数据类型\n",
    "                    value = ds[elem.tag].value\n",
    "                    if isinstance(value, str):\n",
    "                        ds[elem.tag].value = ''\n",
    "                    elif isinstance(value, (int, float, bool)):\n",
    "                        ds[elem.tag].value = None\n",
    "                    else:\n",
    "                        # 对于其他复杂数据类型，可以设置为空字节流或空数据\n",
    "                        ds[elem.tag].value = b'' if isinstance(value, bytes) else None\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing tag {elem.tag}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5550c7d0-ccee-482d-a6bc-e4f233033e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_offsets_from_csv(shift_csv_path):\n",
    "    offsets = {}\n",
    "    with open(shift_csv_path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader, None) \n",
    "        for row in reader:\n",
    "            if len(row) != 2:\n",
    "                continue \n",
    "            patient_id, offset_str = row\n",
    "            try:\n",
    "                offset = int(offset_str)\n",
    "                offsets[patient_id] = offset\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Invalid offset value '{offset_str}' for PatientID '{patient_id}'. Skipping.\")\n",
    "    return offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6d955d7-dc9b-4a88-95d1-be49f0ae6690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_tags(ds, tags, vr='LO'):\n",
    "    for tag in tags:\n",
    "        tag_tuple = (tag >> 16, tag & 0xFFFF) \n",
    "        \n",
    "        if tag_tuple not in ds:\n",
    "            ds.add_new(tag_tuple, vr, '')\n",
    "            print(f\"Added missing tag {tag_tuple} with an empty value and VR '{vr}' to dataset.\")\n",
    "        else:\n",
    "            print(f\"Tag {tag_tuple} already exists in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e4fba99-9268-49b9-b6e6-84e80feef354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_datetime(date_str, days):\n",
    "    \"\"\"Shift the datetime or date by a certain number of days.\"\"\"\n",
    "    try:\n",
    "        # Ensure 'days' is an integer\n",
    "        if not isinstance(days, int):\n",
    "            raise ValueError(f\"Date increment should be an integer, got {type(days)}\")\n",
    "        \n",
    "        if isinstance(date_str, str):\n",
    "            if len(date_str) == 8:  # Format YYYYMMDD\n",
    "                dt = datetime.strptime(date_str, '%Y%m%d')\n",
    "                shifted_dt = dt - timedelta(days=days)  \n",
    "                return shifted_dt.strftime('%Y%m%d')\n",
    "            elif len(date_str) == 14:  # Format YYYYMMDDHHMMSS\n",
    "                dt = datetime.strptime(date_str, '%Y%m%d%H%M%S')\n",
    "                shifted_dt = dt - timedelta(days=days)\n",
    "                return shifted_dt.strftime('%Y%m%d%H%M%S')\n",
    "            elif len(date_str) == 17:  # Format YYYYMMDDHHMMSS.FF\n",
    "                date_part = date_str[:14]  # 日期时间部分\n",
    "                fractional_part = date_str[14:]  # 小数部分\n",
    "                dt = datetime.strptime(date_part, '%Y%m%d%H%M%S')\n",
    "                shifted_dt = dt - timedelta(days=days)\n",
    "                return shifted_dt.strftime('%Y%m%d%H%M%S') + fractional_part\n",
    "            elif len(date_str) == 10 and date_str.isdigit():  # Format Unix timestamp\n",
    "                timestamp = int(date_str) \n",
    "                dt = datetime.utcfromtimestamp(timestamp)\n",
    "                shifted_dt = dt - timedelta(days=days)\n",
    "                return str(int(shifted_dt.timestamp()))\n",
    "            else:\n",
    "                print(f\"Unknown date format: {date_str}\") \n",
    "                return date_str  # Return original if format is unknown\n",
    "        elif isinstance(date_str, int):  # Handle integer input (e.g., Unix timestamp)\n",
    "            dt = datetime.utcfromtimestamp(date_str)\n",
    "            shifted_dt = dt + timedelta(days=days)\n",
    "            return str(int(shifted_dt.timestamp()))  \n",
    "        else:\n",
    "            print(f\"Unhandled date input type: {type(date_str)}\")  \n",
    "            return date_str  # Return original if format is unknown\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError processing date string '{date_str}': {e}\")\n",
    "        return date_str  # Return original if date parsing fails\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing date string '{date_str}': {e}\")\n",
    "        return date_str  # Return original if an unexpected error occurs\n",
    "\n",
    "def update_tag(ds, tag, date_increment):\n",
    "    if tag in ds:\n",
    "        old_value = ds[tag].value\n",
    "\n",
    "        # Ensure date_increment is an integer\n",
    "        if not isinstance(date_increment, int):\n",
    "            print(f\"Invalid date increment: {date_increment}\")\n",
    "            return\n",
    "\n",
    "        # Ensure old_value is processed as string if necessary\n",
    "        if isinstance(old_value, int):\n",
    "            old_value = str(old_value)\n",
    "        elif not isinstance(old_value, str):\n",
    "            return\n",
    "\n",
    "        print(f\"Original value for tag {tag}: {old_value}\") \n",
    "        new_value = shift_datetime(old_value, date_increment)\n",
    "        print(f\"New value for tag {tag}: {new_value}\")  \n",
    "\n",
    "        if isinstance(ds[tag].value, str):\n",
    "            ds[tag].value = new_value\n",
    "        elif isinstance(ds[tag].value, int) and new_value.isdigit():\n",
    "            ds[tag].value = int(new_value)\n",
    "        else:\n",
    "            print(f\"Cannot set new value for tag {tag}: {new_value}\")\n",
    "\n",
    "def process_item(item, tags_to_shift, date_increment):\n",
    "    if isinstance(item, Dataset):\n",
    "        for tag in tags_to_shift:\n",
    "            update_tag(item, tag, date_increment)\n",
    "        for element in item:\n",
    "            if isinstance(element.value, (Dataset, Sequence)):\n",
    "                process_item(element.value, tags_to_shift, date_increment)\n",
    "    elif isinstance(item, Sequence):\n",
    "        for sub_item in item:\n",
    "            process_item(sub_item, tags_to_shift, date_increment)\n",
    "\n",
    "def shift_dates(ds, tags_to_shift, patient_offsets):\n",
    "    try:\n",
    "        if (0x0010, 0x0020) in ds:\n",
    "            patient_id = ds[(0x0010, 0x0020)].value\n",
    "            date_increment = patient_offsets.get(patient_id, 0)\n",
    "            for tag in tags_to_shift:\n",
    "                if tag in ds:\n",
    "                    if isinstance(ds[tag].value, (Dataset, Sequence)):\n",
    "                        process_item(ds[tag].value, tags_to_shift, date_increment)\n",
    "                    else:\n",
    "                        update_tag(ds, tag, date_increment)\n",
    "    except Exception as e:\n",
    "        print(f\"Error shifting dates: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1df77014-2694-496e-a030-946ed30da2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_uid(uid):\n",
    "    hashed_uid = hashlib.sha256(uid.encode()).hexdigest()\n",
    "    numeric_hash = int(hashed_uid, 16)\n",
    "    numeric_hash_str = str(numeric_hash).zfill(19)[:19]\n",
    "    return numeric_hash_str\n",
    "\n",
    "def hash_sequence(ds, tags_to_hash, uid_root, uid_mapping, patient_id=None):\n",
    "    if isinstance(ds, pydicom.Dataset):\n",
    "        for tag in tags_to_hash:\n",
    "            if tag in ds and ds[tag].value:\n",
    "                if isinstance(ds[tag].value, pydicom.sequence.Sequence):\n",
    "                    for item in ds[tag].value:\n",
    "                        hash_sequence(item, tags_to_hash, uid_root, uid_mapping, patient_id)\n",
    "                else:\n",
    "                    # Hash the UID value\n",
    "                    original_uid = ds[tag].value\n",
    "\n",
    "                    # Define the UID root with patient ID\n",
    "                    uid_root_with_patient = f\"{uid_root}{patient_id}.8.117.\"\n",
    "\n",
    "                    # Check if original_uid is already in the uid_mapping\n",
    "                    if original_uid in uid_mapping:\n",
    "                        # Use the existing mapped value and extract the last 19 characters\n",
    "                        new_uid = uid_mapping[original_uid]\n",
    "                        ds[tag].value = new_uid\n",
    "                    else:\n",
    "                        # Generate a new UID using the hash_uid function\n",
    "                        new_uid = hash_uid(original_uid)\n",
    "                        # Construct the full new UID\n",
    "                        full_new_uid = f\"{uid_root_with_patient}{new_uid}\"\n",
    "                        # Update the uid_mapping\n",
    "                        uid_mapping[original_uid] = full_new_uid\n",
    "                        ds[tag].value = f\"{uid_root_with_patient}{new_uid}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c93f8675-56da-4ee3-b537-a57db5e61e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_uid_mapping(uid_mapping, file_path):\n",
    "    with open(file_path, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['id_old', 'id_new'])\n",
    "        for original_uid, hashed_uid in uid_mapping.items():\n",
    "            # Check if hashed_uid already exists as a key (to avoid conflicts)\n",
    "            if hashed_uid in uid_mapping:\n",
    "                print(f\"Warning: {hashed_uid} already exists in uid_mapping.\")\n",
    "            else:\n",
    "                writer.writerow([original_uid, hashed_uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea6f5a92-17c9-4118-bef2-d6ebf6f052fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义正则表达式以识别敏感信息\n",
    "NUMBER1_PATTERN = re.compile(r'\\b\\d{3}-\\d{2}-\\d{4}\\b')\n",
    "date_pattern = re.compile(r'(19|20)\\d{6}')        # 日期格式（例如20160730）固定前两位19/20\n",
    "NAME1_PATTERN = re.compile(r'\\bfor [A-Z][a-z]+\\s[A-Z][a-z]+\\b')   # 匹配两个首字母大写的人名  限制只有首位大写 /两个全大写人名^\n",
    "NAME2_PATTERN = re.compile(r'\\b[A-Z][a-z]+\\s[A-Z][a-z]+ :')\n",
    "#NAME2_PATTERN = re.compile(r'\\b[A-Z]+\\^[A-Z]+\\b')\n",
    "#UPPERCASE_WORD_PATTERN = re.compile(r'\\b[A-Z]+\\b')  # 匹配全大写字母的单词 写具体的字符串\n",
    "#PID_PATTERN = re.compile(r'\\b\\d{9}\\b|\\b\\d{10}\\b')  # 匹配九位数和十位数，把当前pid提取出来\n",
    "\n",
    "PHONE1_PATTERN = re.compile(r'\\d{1}-\\d{3}-\\d{3}-\\d{4}x\\d{1}')\n",
    "PHONE2_PATTERN = re.compile(r'\\(\\d{3}\\)\\d{3}-\\d{4}x\\d{2}')           #(区号)交换机号-分机号\n",
    "PHONE3_PATTERN = re.compile(r'\\(\\d{3}\\)\\d{3}-\\d{4}')\n",
    "PHONE4_PATTERN = re.compile(r'\\d{3}.\\d{3}.\\d{4}x\\d{3}')              #(区号)交换机号-分机号\n",
    "PHONE5_PATTERN = re.compile(r'\\+\\d{1}-\\d{3}-\\d{3}-\\d{4}')             #在5之前识别\n",
    "PHONE6_PATTERN = re.compile(r'\\b\\d{1}-\\d{3}-\\d{3}-\\d{4}\\b')\n",
    "PHONE7_PATTERN = re.compile(r'\\d{3}.\\d{3}.\\d{4}')\n",
    "PHONE8_PATTERN = re.compile(r'\\d{3}-\\d{3}-\\d{4}')\n",
    "PHONE9_PATTERN = re.compile(r'call \\d{10}')\n",
    "\n",
    "ABS1_PATTERN = re.compile(r'\\bat\\s[A-Z]+\\b')                             #by AH替换为 by/at\n",
    "ABS2_PATTERN = re.compile(r'\\bby\\s[A-Z]+\\b')\n",
    "\n",
    "ADDRESS_PATTERN1 = re.compile(r'\\d+ [A-Za-z0-9\\s]+, [A-Z]{2} \\d{5}')\n",
    "ADDRESS_PATTERN2 = re.compile(r'[A-Za-z0-9\\s]+, [A-Z]{2} \\d{5}')\n",
    "#最终版：\n",
    "HISTORY1_PATTERN = re.compile(r'\\b\\w+,\\s\\w+\\sand\\s\\w+\\sMedical Center\\b')     #Aa, Bb and Cc Medical Center\n",
    "HISTORY2_PATTERN = re.compile(r'\\b\\w+\\sand\\s\\w+\\sMedical Center\\b')           #Aa and Bb Medical Center\n",
    "HISTORY3_PATTERN = re.compile(r'\\b[A-Z][a-z]+-[A-Z][a-z]+ Medical Center\\b')  #Aa-Bb Medical Center\n",
    "HISTORY4_PATTERN = re.compile(r'\\b[A-Z][a-z]+ Medical Center\\b')              #Aa Medical Center\n",
    "HISTORY5_PATTERN = re.compile(r'\\b\\w+,\\s\\w+\\sand\\s\\w+\\sMedical Clinic\\b')    #Aa, Bb and Cc Medical Clinic\n",
    "HISTORY6_PATTERN = re.compile(r'\\b\\w+,\\s\\w+\\sand\\s\\w+\\sGeneral Hospital\\b')     #Aa, Bb and Cc General Hospital\n",
    "HISTORY7_PATTERN = re.compile(r'\\b\\w+\\sand\\s\\w+\\sGeneral Hospital\\b')           #Aa and Bb General Hospital\n",
    "HISTORY8_PATTERN = re.compile(r'\\b[A-Z][a-z]+-[A-Z][a-z]+ General Hospital\\b')  #Aa-Bb General Hospital\n",
    "HISTORY9_PATTERN = re.compile(r'\\b[A-Z][a-z]+ General Hospital\\b')              #Aa General Hospital\n",
    "HISTORY10_PATTERN = re.compile(r'\\b\\w+,\\s\\w+\\sand\\s\\w+\\sCommunity Clinic\\b')       #Aa, Bb and Cc Community Clinic\n",
    "HISTORY11_PATTERN = re.compile(r'\\b\\w+\\sand\\s\\w+\\sCommunity Clinic\\b')             #Aa and Bb Community Clinic\n",
    "HISTORY12_PATTERN = re.compile(r'\\b[A-Z][a-z]+-[A-Z][a-z]+ Community Clinic\\b')   #Aa-Bb Community Clinic\n",
    "HISTORY13_PATTERN = re.compile(r'\\b\\w+,\\s\\w+\\sand\\s\\w+\\sCommunity Hospital\\b')      #Aa, Bb and Cc Community Hospital\n",
    "HISTORY14_PATTERN = re.compile(r'\\b\\w+\\sand\\s\\w+\\sCommunity Hospital\\b')             #Aa and Bb Community Hospital\n",
    "HISTORY15_PATTERN = re.compile(r'\\b[A-Z][a-z]+-[A-Z][a-z]+ Community Hospital\\b')   #Aa-Bb Community Hospital\n",
    "HISTORY16_PATTERN = re.compile(r'\\b\\w+,\\s\\w+\\sand\\s\\w+\\sMemorial\\b')    #Aa, Bb and Cc Memorial\n",
    "HISTORY17_PATTERN = re.compile(r'\\b\\w+\\sand\\s\\w+\\sMemorial\\b')          #Aa and Bb Memorial\n",
    "HISTORY18_PATTERN = re.compile(r'\\b[A-Z][a-z]+-[A-Z][a-z]+ Memorial\\b')  #Aa-Bb Memorial\n",
    "HISTORY19_PATTERN = re.compile(r'\\b[A-Z][a-z]+\\s[A-Z][a-z]+ Memorial\\b') #Aa Bb Memorial\n",
    "HISTORY20_PATTERN = re.compile(r'\\b[A-Z][a-z]+ Memorial\\b')             #Aa Memorial\n",
    "HISTORY21_PATTERN = re.compile(r'\\b\\w+,\\s\\w+\\sand\\s\\w+\\sGeneral\\b')    #Aa, Bb and Cc General\n",
    "HISTORY22_PATTERN = re.compile(r'\\b\\w+\\sand\\s\\w+\\sGeneral\\b')          #Aa and Bb General\n",
    "HISTORY23_PATTERN = re.compile(r'\\b[A-Z][a-z]+-[A-Z][a-z]+ General\\b') #Aa-Bb General\n",
    "HISTORY24_PATTERN = re.compile(r'\\b[A-Z][a-z]+ General\\b')             #Aa General\n",
    "HISTORY25_PATTERN = re.compile(r'\\b\\w+,\\s\\w+\\sand\\s\\w+\\sClinic\\b')       #Aa, Bb and Cc Clinic\n",
    "HISTORY26_PATTERN = re.compile(r'\\b[A-Z][a-z]+\\s[A-Z][a-z]+ Clinic\\b')   #Aa Bb Clinic\n",
    "HISTORY27_PATTERN = re.compile(r'\\b[A-Z][a-z]+-[A-Z][a-z]+ Clinic\\b')    #Aa-Bb Clinic\n",
    "HISTORY28_PATTERN = re.compile(r'\\b[A-Z][a-z]+ Clinic\\b')               #Aa Clinic\n",
    "HISTORY29_PATTERN = re.compile(r'\\b\\w+,\\s\\w+\\sand\\s\\w+\\sHospital\\b')    #Aa, Bb and Cc Hospital\n",
    "HISTORY30_PATTERN = re.compile(r'\\b\\w+\\sand\\s\\w+\\sHospital\\b')          #Aa and Bb Hospital\n",
    "HISTORY31_PATTERN = re.compile(r'\\b[A-Z][a-z]+\\s[A-Z][a-z]+ Hospital\\b')   #Aa Bb Hospital\n",
    "HISTORY32_PATTERN = re.compile(r'\\b[A-Z][a-z]+-[A-Z][a-z]+ Hospital\\b')    #Aa-Bb Hospital\n",
    "HISTORY33_PATTERN = re.compile(r'\\b[A-Z][a-z]+ Hospital\\b')               #Aa Hospital\n",
    "\n",
    "DOCTOR1_PATTERN = re.compile(r'\\bDr\\.\\s[A-Z][a-z]+\\b')\n",
    "DOCTOR2_PATTERN = re.compile(r'\\bDR\\.[A-Z]+\\b')\n",
    "DOCTOR3_PATTERN = re.compile(r'DR_[A-Z]+')\n",
    "\n",
    "str1 = \"% green/purple high\"\n",
    "str2 = \"% white high\"\n",
    "str3 = \"3792105090\"\n",
    "str4 = \"2929551111\"\n",
    "str5 = \"5932656543\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e681fa67-10e8-4943-9bae-2581b7697f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sensitive_info_from_value(value, patient_id):\n",
    "    if isinstance(value, str):  \n",
    "        # 处理敏感信息\n",
    "        if(value in (\"AL\",\"CW\",\"CH\",\"RD\",\"RS\",\"DL\",\"KM\")):\n",
    "            value=''\n",
    "        value = value.replace(patient_id, \"\")   #提前，防止误删成日期\n",
    "        value = ADDRESS_PATTERN1.sub('', value)\n",
    "        value = ADDRESS_PATTERN2.sub('', value)\n",
    "        value = HISTORY1_PATTERN.sub('', value)\n",
    "        value = HISTORY2_PATTERN.sub('', value)\n",
    "        value = HISTORY3_PATTERN.sub('', value)\n",
    "        value = HISTORY4_PATTERN.sub('', value)\n",
    "        value = HISTORY5_PATTERN.sub('', value)\n",
    "        value = HISTORY6_PATTERN.sub('', value)\n",
    "        value = HISTORY7_PATTERN.sub('', value)\n",
    "        value = HISTORY8_PATTERN.sub('', value)\n",
    "        value = HISTORY9_PATTERN.sub('', value)\n",
    "        value = HISTORY10_PATTERN.sub('', value)\n",
    "        value = HISTORY11_PATTERN.sub('', value)\n",
    "        value = HISTORY12_PATTERN.sub('', value)\n",
    "        value = HISTORY13_PATTERN.sub('', value)\n",
    "        value = HISTORY14_PATTERN.sub('', value)\n",
    "        value = HISTORY15_PATTERN.sub('', value)\n",
    "        value = HISTORY16_PATTERN.sub('', value)\n",
    "        value = HISTORY17_PATTERN.sub('', value)\n",
    "        value = HISTORY18_PATTERN.sub('', value)\n",
    "        value = HISTORY19_PATTERN.sub('', value)\n",
    "        value = HISTORY20_PATTERN.sub('', value)\n",
    "        value = HISTORY21_PATTERN.sub('', value)\n",
    "        value = HISTORY22_PATTERN.sub('', value)\n",
    "        value = HISTORY23_PATTERN.sub('', value)\n",
    "        value = HISTORY24_PATTERN.sub('', value)\n",
    "        value = HISTORY25_PATTERN.sub('', value)\n",
    "        value = HISTORY26_PATTERN.sub('', value)\n",
    "        value = HISTORY27_PATTERN.sub('', value)\n",
    "        value = HISTORY28_PATTERN.sub('', value)\n",
    "        value = HISTORY29_PATTERN.sub('', value)\n",
    "        value = HISTORY30_PATTERN.sub('', value) \n",
    "        value = HISTORY31_PATTERN.sub('', value) \n",
    "        value = HISTORY32_PATTERN.sub('', value) \n",
    "        value = HISTORY33_PATTERN.sub('', value) \n",
    "        value = NUMBER1_PATTERN.sub('', value)\n",
    "        value = date_pattern.sub('', value)\n",
    "        value = NAME1_PATTERN.sub('for', value)\n",
    "        value = NAME2_PATTERN.sub(':', value) \n",
    "        value = PHONE1_PATTERN.sub('', value)\n",
    "        value = PHONE2_PATTERN.sub('', value)\n",
    "        value = PHONE3_PATTERN.sub('', value)\n",
    "        value = PHONE4_PATTERN.sub('', value)\n",
    "        value = PHONE5_PATTERN.sub('', value)\n",
    "        value = PHONE6_PATTERN.sub('', value)\n",
    "        value = PHONE7_PATTERN.sub('', value)\n",
    "        value = PHONE8_PATTERN.sub('', value)\n",
    "        value = PHONE9_PATTERN.sub('call', value)\n",
    "        value = DOCTOR1_PATTERN.sub('', value)\n",
    "        value = DOCTOR2_PATTERN.sub('', value)\n",
    "        value = DOCTOR3_PATTERN.sub('', value)\n",
    "        value = ABS1_PATTERN.sub('at', value)\n",
    "        value = ABS2_PATTERN.sub('by', value)\n",
    "        value = value.replace(str1, \"\")\n",
    "        value = value.replace(str2, \"\")\n",
    "        value = value.replace(str3, \"\")\n",
    "        value = value.replace(str4, \"\")\n",
    "        value = value.replace(str5, \"\")\n",
    "    \n",
    "    elif isinstance(value, IS):\n",
    "        # 将 IS 类型值转换为字符串进行处理\n",
    "        value = str(value)\n",
    "        value = remove_sensitive_info_from_value(value, patient_id)\n",
    "    return value\n",
    "\n",
    "def remove_sensitive_info(ds, sensitive_tags, patient_id):\n",
    "    def clear_value(value):\n",
    "        if isinstance(value, Sequence):\n",
    "            for item in value:\n",
    "                if isinstance(item, Dataset):\n",
    "                    remove_sensitive_info(item, sensitive_tags, patient_id)\n",
    "        elif isinstance(value, Dataset):\n",
    "            remove_sensitive_info(value, sensitive_tags, patient_id)\n",
    "        elif isinstance(value, (str, IS, list)):\n",
    "            return remove_sensitive_info_from_value(value, patient_id)\n",
    "        return value\n",
    "\n",
    "    for tag in sensitive_tags:\n",
    "        tag = pydicom.tag.Tag(tag)  \n",
    "        if tag in ds:\n",
    "            ds[tag].value = clear_value(ds[tag].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abda2ef6-75f2-4f16-9051-67a648acdaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deid_dicom_directory(input_directory, output_directory, tags_to_clear, id_lookup_file, shift_csv_path, tags_to_shift, tags_to_hash, uid_root, tags_to_add):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    uid_mapping = {}\n",
    "    \n",
    "    # Load ID mapping and date offsets\n",
    "    id_lookup = {}\n",
    "    with open(id_lookup_file, 'r') as f:\n",
    "        for line in f:\n",
    "            original_id, new_id = line.strip().split(',')\n",
    "            id_lookup[original_id] = new_id\n",
    "    \n",
    "    patient_offsets = load_offsets_from_csv(shift_csv_path)\n",
    "    \n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.dcm'):\n",
    "                input_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(input_path, input_directory)\n",
    "                output_path = os.path.join(output_directory, relative_path)\n",
    "\n",
    "                # Create output directory if it does not exist\n",
    "                os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "                ds = pydicom.dcmread(input_path, force=True)\n",
    "\n",
    "                patient_id = ds.PatientID\n",
    "                #part remove\n",
    "                remove_sensitive_info(ds, sensitive_tags,patient_id)\n",
    "                \n",
    "                # Apply ID mapping\n",
    "                apply_id_mapping(ds, id_lookup)\n",
    "                modify_patient_name_to_id(ds)\n",
    "                \n",
    "                # Extract patient ID for UID hashing\n",
    "                patient_id = ds.PatientID if 'PatientID' in ds else 'UnknownPatientID'\n",
    "                uid_root = f\"1.2.397.4.5.\"\n",
    "\n",
    "                # Clear specified tags\n",
    "                clear_tags(ds, tags_to_clear)\n",
    "                \n",
    "                # Shift dates based on patient offsets\n",
    "                shift_dates(ds, tags_to_shift, patient_offsets)\n",
    "\n",
    "                # Check if file meta information exists and update (0002,0003) tag if present\n",
    "                \n",
    "                if ds.file_meta and 'MediaStorageSOPInstanceUID' in ds.file_meta:\n",
    "                    original_uid = ds.file_meta.MediaStorageSOPInstanceUID\n",
    "                    new_uid = hash_uid(original_uid)\n",
    "                    ds.file_meta.MediaStorageSOPInstanceUID = f\"{uid_root}{patient_id}.8.117.{new_uid}\" #修改，新增{patient_id}.\n",
    "                    uid_mapping[original_uid] = ds.file_meta.MediaStorageSOPInstanceUID\n",
    "                \n",
    "                \n",
    "                # Hash other specified tags in the dataset and recursively process sequences\n",
    "                hash_sequence(ds, tags_to_hash, uid_root, uid_mapping, patient_id)\n",
    "\n",
    "                # Add the tag if it does not exist\n",
    "                add_missing_tags(ds, tags_to_add)\n",
    "\n",
    "                # Set empty tag values to \"add\" for tags in `tag_not_null`\n",
    "                set_empty_tags_to_add(ds, tag_not_null)\n",
    "                \n",
    "                \n",
    "\n",
    "                # Save modified DICOM file\n",
    "                ds.save_as(output_path)\n",
    "                print(f\"Processed file saved: {output_path}\")\n",
    "\n",
    "    print(\"UID Mapping Contents:\")\n",
    "    for original_uid, new_uid in uid_mapping.items():\n",
    "        print(f\"Original UID: {original_uid}, New UID: {new_uid}\")\n",
    "    # Save the UID mapping to a CSV file\n",
    "    save_uid_mapping(uid_mapping, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea0340e3-ffdc-4960-946a-6055e8c4e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"C:/Users/珊珊/Desktop/test/input\"\n",
    "id_lookup_file =  \"C:/Users/珊珊/Desktop/test/patient_id_mapping.csv\"\n",
    "output_dir = \"C:/Users/珊珊/Desktop/test/output\"\n",
    "csv_path = 'C:/Users/珊珊/Desktop/test/shift.csv'\n",
    "uid_root=\"1.2.375.4.5.\"\n",
    "file_path='C:/Users/珊珊/Desktop/test/uid_mapping.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fa7e5ba-8ea9-46f2-a602-c1f969c3b75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original value for tag 524306: 20161013\n",
      "New value for tag 524306: 20160930\n",
      "Original value for tag 524320: 20161013\n",
      "New value for tag 524320: 20160930\n",
      "Original value for tag 524321: 20161013\n",
      "New value for tag 524321: 20160930\n",
      "Original value for tag 524322: 20161013\n",
      "New value for tag 524322: 20160930\n",
      "Original value for tag 524323: \n",
      "Unknown date format: \n",
      "New value for tag 524323: \n",
      "Original value for tag 524330: 20161013125441\n",
      "New value for tag 524330: 20160930125441\n",
      "Original value for tag 4194884: 20161013\n",
      "New value for tag 4194884: 20160930\n",
      "Added missing tag (8, 104) with an empty value and VR 'LO' to dataset.\n",
      "Added missing tag (24, 96) with an empty value and VR 'LO' to dataset.\n",
      "Tag (24, 20737) already exists in the dataset.\n",
      "Added missing tag (24, 28676) with an empty value and VR 'LO' to dataset.\n",
      "Added missing tag (32, 18) with an empty value and VR 'LO' to dataset.\n",
      "Added missing tag (32, 4160) with an empty value and VR 'LO' to dataset.\n",
      "Tag (8272, 32) already exists in the dataset.\n",
      "Original value for tag (0008, 0068): \n",
      "Modified value for tag (0008, 0068) to 'NA'\n",
      "Original value for tag (2050, 0020): IDENTITY\n",
      "Processed file saved: C:/Users/珊珊/Desktop/output\\2861.dcm\n",
      "UID Mapping Contents:\n",
      "Original UID: 2.3.233.1.0.8515474.5.043.2571198004491253064, New UID: 1.2.397.4.5.0000035.8.117.6348304806965517499\n",
      "Original UID: 2.3.233.1.0.8515474.5.043.1618769542927414299, New UID: 1.2.397.4.5.0000035.8.117.2091674185417635862\n",
      "Original UID: 2.3.233.1.0.8515474.5.043.3751234163313928298, New UID: 1.2.397.4.5.0000035.8.117.7970645905433128852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\珊珊\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydicom\\valuerep.py:443: UserWarning: Invalid value for VR UI: '2.3.233.1.0.8515474.5.043.2571198004491253064'. Please see <https://dicom.nema.org/medical/dicom/current/output/html/part05.html#table_6.2-1> for allowed values for each VR.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\珊珊\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydicom\\valuerep.py:443: UserWarning: Invalid value for VR UI: '2.3.233.1.0.8515474.5.043.1618769542927414299'. Please see <https://dicom.nema.org/medical/dicom/current/output/html/part05.html#table_6.2-1> for allowed values for each VR.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\珊珊\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydicom\\valuerep.py:443: UserWarning: Invalid value for VR UI: '2.3.233.1.0.8515474.5.043.3751234163313928298'. Please see <https://dicom.nema.org/medical/dicom/current/output/html/part05.html#table_6.2-1> for allowed values for each VR.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\珊珊\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydicom\\valuerep.py:443: UserWarning: Invalid value for VR UI: '1.2.397.4.5.0000035.8.117.6348304806965517499'. Please see <https://dicom.nema.org/medical/dicom/current/output/html/part05.html#table_6.2-1> for allowed values for each VR.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\珊珊\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydicom\\valuerep.py:443: UserWarning: Invalid value for VR UI: '1.2.397.4.5.0000035.8.117.2091674185417635862'. Please see <https://dicom.nema.org/medical/dicom/current/output/html/part05.html#table_6.2-1> for allowed values for each VR.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\珊珊\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydicom\\valuerep.py:443: UserWarning: Invalid value for VR UI: '1.2.397.4.5.0000035.8.117.7970645905433128852'. Please see <https://dicom.nema.org/medical/dicom/current/output/html/part05.html#table_6.2-1> for allowed values for each VR.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "deid_dicom_directory(directory, output_dir, tags_to_clear, id_lookup_file, csv_path, tags_to_shift, tags_to_hash, uid_root,tag_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f2006-8bdd-4151-aefa-558a4090b293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
